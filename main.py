from enum import Enum
import logging

from fastapi import FastAPI, HTTPException
from starlette.concurrency import run_in_threadpool
from pydantic import BaseModel, Field
import uvicorn
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


# ------------------------------------------------------------------------------
# Logging Setup
# ------------------------------------------------------------------------------
# Configure logging to include timestamps, log level, and message.
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# Enums and Pydantic Models
# ------------------------------------------------------------------------------
class DeviceEnum(str, Enum):
    cpu = "cpu"
    cuda = "cuda"
    mps = "mps"

# Define a request schema using Pydantic
class TextGenerationPayload(BaseModel):
    """Payload for text generation parameters."""

    prompt: str = Field(
        ...,
        min_length=1,
        example="Hello, I'm an LLM",
        description="The text prompt to continue from."
    )
    max_length: int = Field(
        50,
        gt=0,
        le=1000,
        description="Maximum number of tokens to generate."
    )
    temperature: float = Field(
        1.0,
        ge=0.0,
        le=2.0,
        description="Sampling temperature. Higher values produce more random outputs."
    )
    top_k: int = Field(
        50,
        ge=0,
        le=1000,
        description="Top-k filtering parameter."
    )
    top_p: float = Field(
        1.0,
        ge=0.0,
        le=1.0,
        description="Top-p (nucleus) sampling parameter."
    )
    do_sample: bool = Field(
        True,
        description="Enable sampling. If false, greedy search is used."
    )
    device: DeviceEnum = Field(
        DeviceEnum.cpu,
        description="Select between CPU, CUDA, or MPS for inference."
    )

# Response model
class TextGenerationResponse(BaseModel):
    generated_text: str = Field(..., description="The text generated by GPT-2.")

# ------------------------------------------------------------------------------
# Model Initialization
# ------------------------------------------------------------------------------
# Preload the tokenizer and models per device in a dictionary.
logger.info("Loading tokenizer and model(s)...")
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
models = {}

# Always default to loading the CPU version.
models[DeviceEnum.cpu] = AutoModelForCausalLM.from_pretrained("openai-community/gpt2").to("cpu")
logger.info("Loaded model on CPU.")

# Optionally load CUDA and MPS versions if available.
if torch.cuda.is_available():
    models[DeviceEnum.cuda] = AutoModelForCausalLM.from_pretrained("openai-community/gpt2").to("cuda")
    logger.info("CUDA is available: Loaded model on CUDA.")
if torch.backends.mps.is_available():
    models[DeviceEnum.mps] = AutoModelForCausalLM.from_pretrained("openai-community/gpt2").to("mps")
    logger.info("MPS is available: Loaded model on MPS.")

# ------------------------------------------------------------------------------
# FastAPI Application
# ------------------------------------------------------------------------------
app = FastAPI(
    title="GPT-2 Text Generation API",
    description="An API that generates text using the openai-community/gpt2 model.",
    version="1.0.0"
)

@app.get("/")
def read_root():
    logger.info("Root endpoint was called.")
    return {"message": "Hello, World!"}

@app.post("/predict", response_model=TextGenerationResponse)
async def predict(payload: TextGenerationPayload):
    """
    Generate text from a given prompt using GPT-2.
    """
    logger.info(f"Received prediction request on device: {payload.device} with prompt length: {len(payload.prompt)}")
    if not payload.prompt.strip():
        logger.warning("Received an empty prompt.")
        raise HTTPException(status_code=400, detail="Prompt must not be empty.")
    

    # Check that the requested device is available.
    if payload.device not in models:
        error_msg = f"{payload.device} is not available."
        logger.error(error_msg)
        raise HTTPException(status_code=400, detail=error_msg)

    model = models[payload.device]
    device_str = payload.device.value  # e.g., "cpu", "cuda", or "mps"

    input_ids = tokenizer.encode(payload.prompt, return_tensors="pt").to(device_str)

    if payload.max_length <= input_ids.shape[1]:
        error_msg = "max_length must exceed prompt length."
        logger.error(error_msg)
        raise HTTPException(status_code=400, detail=error_msg)
    
    try:
        logger.info("Starting text generation.")
        output_ids = await run_in_threadpool(
            model.generate,
            input_ids,
            max_length=payload.max_length,
            temperature=payload.temperature,
            top_k=payload.top_k,
            top_p=payload.top_p,
            do_sample=payload.do_sample
        )
        logger.info("Text generation completed successfully.")
    except Exception as e:
        error_msg = f"Error during text generation: {str(e)}"
        logger.exception(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    logger.info("Returning generated text.")
    return {"generated_text": generated_text}

# ------------------------------------------------------------------------------
# Application Entry Point
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)